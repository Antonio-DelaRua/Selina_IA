import asyncio
from langchain_ollama import OllamaLLM  # Para usar CodeLlama en local
from model import HistoryEntry, PythonDB
from info import CompanyInfo
from last_history import *

# üöÄ Cargar el modelo CodeLlama en local
local_llm = OllamaLLM(
    model="codellama:latest",
    temperature=0.3,
    num_predict=700,
    repeat_penalty=1.2,
    num_gpu_layers=20,
)

async def chat_with_codellama(prompt):
    """Llama a CodeLlama en local de forma as√≠ncrona para evitar bloqueos."""
    try:
        loop = asyncio.get_running_loop()
        response = await loop.run_in_executor(None, local_llm.invoke, prompt)
        return response or "‚ö†Ô∏è No pude generar una respuesta. Int√©ntalo de nuevo."
    except Exception as e:
        print(f"‚ùå Error al llamar a CodeLlama: {e}")
        return f"Error al llamar a CodeLlama: {e}"

async def agent(prompt):
    user_query = prompt.lower().strip()

    # ‚úÖ Si el usuario pide abrir el historial, lo abrimos y terminamos
    if user_query in ["abrir historial", "ver historial", "historial"]:
        return abrir_historial()

    prompt_template = f"""
    **Modo Consultor√≠a T√©cnica - Selina**  
    Eres Selina, experta en Python y arquitectura de software para {CompanyInfo.NOMBRE}. 

    **Directrices Estrictas de Formato:**
    - Responder siempre en espa√±ol, excepci√≥n: que se te indique lo contrario
    - Prohibido usar t√≠tulos como "Secci√≥n X" o "Tema Principal"
    - Usar solo emojis como separadores de contenido
    - M√°ximo 5 vi√±etas con emojis relevantes
    - C√≥digo en bloques con sintaxis espec√≠fica

    **Ejemplo de Respuesta Esperada:**
    üß† <descripci√≥n t√©cnica clave>  
    üîß <relaci√≥n con arquitectura>  
    üí° <ventaja principal>  
    üö® <consideraci√≥n importante>  
    ```python
    <c√≥digo m√≠nimo enfocado>
    ```

    **Consulta:** {prompt}
    """

    # ‚úÖ B√∫squeda r√°pida en FAQs
    for keyword, answer in CompanyInfo.FAQS.items():
        if keyword in user_query:
            respuesta = f"üìå **Respuesta r√°pida:**\n{answer}"
            guardar_en_txt(prompt, respuesta)  # Guardar en archivo
            return respuesta

    # ‚úÖ Optimizaci√≥n: Consultas en base de datos (evita repeticiones)
    try:
        respuesta = PythonDB.get_by_prompt(prompt) or HistoryEntry.get_by_prompt(prompt)
        if respuesta:
            guardar_en_txt(prompt, respuesta.response)
            return respuesta.response
    except Exception as e:
        print(f"‚ö†Ô∏è Error en la consulta de base de datos: {e}")

    # üî• Generar respuesta con CodeLlama de forma as√≠ncrona
    response = await chat_with_codellama(prompt_template)  

    # ‚úÖ Guardar solo si no existe en historial
    if not HistoryEntry.get_by_prompt(prompt):
        HistoryEntry(prompt=prompt, response=response).save()

    # ‚úÖ Guardar en archivo
    guardar_en_txt(prompt, response)

    return response
